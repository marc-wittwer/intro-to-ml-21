{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "# from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "# import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_epochs = 30  # each epoch is one pass over the whole dataset\n",
    "batch_size = 10  # how many samples per training step (32,64,128)\n",
    "num_workers = 7  # how many CPU cores to use\n",
    "\n",
    "n_features = 1000 # How many features per image\n",
    "\n",
    "\n",
    "# Specify some file names\n",
    "img_features_file = \"data/ing_feature_counts.csv\"\n",
    "train_triplets_file = \"train_triplets.txt\" #\"train_triplets.txt\"\n",
    "test_triplets_file = \"test_triplets.txt\" #\"test_triplets.txt\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FoodTriplets(Dataset):\n",
    "    \"\"\"\n",
    "    Class to load food triplets. Individual items consist of:\n",
    "     - x: concatenated image features of a triplet (ABC)\n",
    "     - y: corresponding label (1: B is more similar, 0: C is more similar, None: no label)\n",
    "    \"\"\"\n",
    "    def __init__(self, features_file, triplets_file, is_labelled_data=False):\n",
    "        print(\"initializing \" + str(triplets_file) + \" dataset...\")\n",
    "        \n",
    "#         self.img_features = pd.read_csv(\"data/\" + features_file, header=None, index_col=0)\n",
    "        # First row is header labels\n",
    "        self.img_features = pd.read_csv(img_features_file, index_col=0, header=0)\n",
    " \n",
    "        self.triplets = pd.read_csv(\"data/\" + triplets_file, sep=\" \", header=None).to_numpy()\n",
    "        self.is_labelled_data = is_labelled_data\n",
    "        self.labels = None\n",
    "\n",
    "        if self.is_labelled_data:\n",
    "            # For each triplet ABC add a triplet ACB\n",
    "            # Also add corresponding labels\n",
    "            dim = self.triplets.shape\n",
    "            extended_triplets = np.zeros((2*dim[0], dim[1]), dtype=np.int32)\n",
    "            self.labels = np.zeros(2*dim[0], dtype=bool)\n",
    "            idx = 0\n",
    "            for triplet in self.triplets:\n",
    "                extended_triplets[idx, :] = triplet\n",
    "                extended_triplets[idx + 1, :] = [triplet[0], triplet[2], triplet[1]]\n",
    "                self.labels[idx] = 1  # label at idx+1 is already initialized to 0\n",
    "                idx += 2\n",
    "\n",
    "            self.triplets = extended_triplets\n",
    "\n",
    "        print(\"done\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        a, b, c = triplet[0], triplet[1], triplet[2]\n",
    "        a_features = self.img_features.loc[[a]].to_numpy(dtype=np.float32)\n",
    "        b_features = self.img_features.loc[[b]].to_numpy(dtype=np.float32)\n",
    "        c_features = self.img_features.loc[[c]].to_numpy(dtype=np.float32)\n",
    "        features = np.squeeze(np.concatenate((a_features, b_features, c_features), axis=1))\n",
    "        label = 0  # dummy label\n",
    "        if self.is_labelled_data:\n",
    "            label = np.array([self.labels[idx]])\n",
    "        return {\"x\": features, \"y\": label}\n",
    "\n",
    "def get_model(device):\n",
    "    return SimpleNetwork().to(device)\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        n_features = 669\n",
    "\n",
    "        nin = 3*n_features\n",
    "        n1 = 2*n_features\n",
    "        n2 = n_features\n",
    "        n3 = int(n_features/2)\n",
    "        n4 = int(n_features/4)\n",
    "        n5 = int(n_features/8)\n",
    "        nout = 1\n",
    "\n",
    "        p_dropout1 = 0.3\n",
    "        p_dropout2 = 0.5\n",
    "        p_dropout3 = 0.5\n",
    "        p_dropout4 = 0.5\n",
    "        p_dropout5 = 0.2\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=669, out_features=333),\n",
    "            nn.BatchNorm1d(333),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout1, inplace=False),\n",
    "            nn.Linear(in_features=333, out_features=165),\n",
    "            nn.BatchNorm1d(165),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout2, inplace=False),\n",
    "            nn.Linear(in_features=165, out_features=80),\n",
    "            nn.BatchNorm1d(80),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout3, inplace=False),\n",
    "            nn.Linear(in_features=80, out_features=40),\n",
    "            nn.BatchNorm1d(40),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout4, inplace=False),\n",
    "            nn.Linear(in_features=40, out_features=1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm1d(125),\n",
    "# #             nn.Dropout(p=p_dropout5, inplace=False),\n",
    "#             nn.Linear(in_features=125, out_features=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "#         self.classifier = nn.Sequential(\n",
    "#             nn.Linear(in_features=nin, out_features=n1),\n",
    "#             nn.BatchNorm1d(n1),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout1, inplace=False),\n",
    "#             nn.Linear(in_features=n1, out_features=n2),\n",
    "#             nn.BatchNorm1d(n2),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout2, inplace=False),\n",
    "#             nn.Linear(in_features=n2, out_features=n3),\n",
    "#             nn.BatchNorm1d(n3),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout3, inplace=False),\n",
    "#             nn.Linear(in_features=n3, out_features=n4),\n",
    "#             nn.BatchNorm1d(n4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout4, inplace=False),\n",
    "#             nn.Linear(in_features=n4, out_features=n5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm1d(n5),\n",
    "#             nn.Dropout(p=p_dropout5, inplace=False),\n",
    "#             nn.Linear(in_features=n5, out_features=nout),\n",
    "#             nn.Sigmoid()\n",
    "#         )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing train_triplets.txt dataset...\n",
      "done\n",
      "initializing test_triplets.txt dataset...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# initialize datasets\n",
    "train_data = FoodTriplets(img_features_file, train_triplets_file, is_labelled_data=True)\n",
    "test_data = FoodTriplets(img_features_file, test_triplets_file, is_labelled_data=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 669])\n"
     ]
    }
   ],
   "source": [
    "# Split train set into train and test set to assess accuracy on unused set\n",
    "l_train = len(train_data)\n",
    "val_size = int(0.8 * l_train + 1)\n",
    "indices = list(range(l_train))\n",
    "np.random.shuffle(indices)\n",
    "val_indices, t_indices = indices[:val_size], indices[val_size:]\n",
    "\n",
    "\n",
    "train_trainloader = DataLoader(torch.utils.data.Subset(train_data, t_indices), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_testloader = DataLoader(torch.utils.data.Subset(train_data, val_indices), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n",
    "for i, data, in enumerate(train_trainloader, 1):\n",
    "    print(data['x'].shape)\n",
    "    break\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def experiment(num_epochs, trainloader, device, model, optimizer, scheduler):\n",
    "    train_results = {}\n",
    "    test_results = {}\n",
    "    # Initial test error\n",
    "    loss, acc, time = test(device, model)\n",
    "    print(f'Upon initialization. [Test] \\t Time {time.avg:.2f} \\\n",
    "            Loss {loss.avg:.2f} \\t Accuracy {acc.avg:.2f}')\n",
    "    test_results[0] = (loss, acc, time)\n",
    "\n",
    "\n",
    "    for epoch in range(1, num_epochs+1):\n",
    "        loss, acc, time = train(trainloader, device, model, optimizer, scheduler)\n",
    "        print(f'Epoch {epoch}. [Train] \\t Time {time.sum:.2f} Loss \\\n",
    "                {loss.avg:.2f} \\t Accuracy {acc.avg:.2f}')\n",
    "        train_results[epoch] = (loss.avg, acc.avg, time.avg)\n",
    "\n",
    "#         if not (epoch % 2):\n",
    "        if True:\n",
    "          loss, acc, time = test(device, model)\n",
    "          print(f'Epoch {epoch}. [Test] \\t Time {time.sum:.2f} T-Loss \\\n",
    "                {loss.avg:.2f} \\t T-Accuracy {acc.avg:.2f}')\n",
    "          test_results[epoch] = (loss.avg, acc.avg, time.avg)\n",
    "\n",
    "def train(trainloader, device, model, optimizer, scheduler):\n",
    "    time_ = AverageMeter()\n",
    "    loss_ = AverageMeter()\n",
    "    acc_ = AverageMeter()\n",
    "    model.train()\n",
    "\n",
    "    for i, data, in enumerate(trainloader, 1):\n",
    "        # Accounting\n",
    "        end = time.time()\n",
    "\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs = data[\"x\"].float()\n",
    "        labels = data[\"y\"].float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        bs = inputs.size(0)\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()  # all the tensors have .grad attribute\n",
    "        # forward propagation\n",
    "        logits = model(inputs) # forward propagation\n",
    "        loss = criterion(logits, labels) # computing the loss for predictions\n",
    "        # Backward propagation\n",
    "        loss.backward() # backpropgation\n",
    "        # Optimization step.\n",
    "        optimizer.step() # applying an optimization step\n",
    "\n",
    "        # Accounting\n",
    "        acc = ((torch.round(logits) == labels).sum().float() / bs).float()\n",
    "        loss_.update(loss.mean().item(), bs)\n",
    "        acc_.update(acc.item(), bs)\n",
    "        time_.update(time.time() - end)\n",
    "\n",
    "#     scheduler.step()\n",
    "    return loss_, acc_, time_\n",
    "\n",
    "def test(device, model):\n",
    "    time_ = AverageMeter()\n",
    "    loss_ = AverageMeter()\n",
    "    acc_ = AverageMeter()\n",
    "    model.eval()\n",
    "\n",
    "    for i, data, in enumerate(train_testloader, 1):\n",
    "        # Accounting\n",
    "        end = time.time()\n",
    "\n",
    "        inputs = data[\"x\"].float()\n",
    "        labels = data[\"y\"].float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        bs = inputs.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            acc = ((torch.round(logits) == labels).sum().float() / bs).float()\n",
    "\n",
    "            # Accounting\n",
    "            loss_.update(loss.mean().item(), bs)\n",
    "            acc_.update(acc.mean().item(), bs)\n",
    "            time_.update(time.time() - end)\n",
    "\n",
    "    return loss_, acc_, time_\n",
    "\n",
    "class AverageMeter():\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = get_model(device)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.01, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Upon initialization. [Test] \t Time 0.00             Loss 0.69 \t Accuracy 0.50\n",
      "Epoch 1. [Train] \t Time 6.37 Loss                 0.65 \t Accuracy 0.63\n",
      "Epoch 1. [Test] \t Time 7.40 T-Loss                 0.61 \t T-Accuracy 0.66\n",
      "Epoch 2. [Train] \t Time 6.48 Loss                 0.63 \t Accuracy 0.65\n",
      "Epoch 2. [Test] \t Time 7.61 T-Loss                 0.61 \t T-Accuracy 0.67\n",
      "Epoch 3. [Train] \t Time 6.52 Loss                 0.62 \t Accuracy 0.66\n",
      "Epoch 3. [Test] \t Time 7.59 T-Loss                 0.61 \t T-Accuracy 0.67\n",
      "Epoch 4. [Train] \t Time 6.40 Loss                 0.62 \t Accuracy 0.66\n",
      "Epoch 4. [Test] \t Time 7.77 T-Loss                 0.60 \t T-Accuracy 0.68\n",
      "Epoch 5. [Train] \t Time 6.49 Loss                 0.61 \t Accuracy 0.67\n",
      "Epoch 5. [Test] \t Time 7.57 T-Loss                 0.59 \t T-Accuracy 0.68\n",
      "Epoch 6. [Train] \t Time 6.72 Loss                 0.60 \t Accuracy 0.68\n",
      "Epoch 6. [Test] \t Time 7.43 T-Loss                 0.59 \t T-Accuracy 0.68\n",
      "Epoch 7. [Train] \t Time 6.35 Loss                 0.60 \t Accuracy 0.68\n",
      "Epoch 7. [Test] \t Time 7.63 T-Loss                 0.59 \t T-Accuracy 0.68\n",
      "Epoch 8. [Train] \t Time 6.70 Loss                 0.59 \t Accuracy 0.68\n",
      "Epoch 8. [Test] \t Time 7.60 T-Loss                 0.59 \t T-Accuracy 0.68\n",
      "Epoch 9. [Train] \t Time 6.56 Loss                 0.58 \t Accuracy 0.69\n",
      "Epoch 9. [Test] \t Time 7.66 T-Loss                 0.59 \t T-Accuracy 0.68\n",
      "Epoch 10. [Train] \t Time 6.60 Loss                 0.58 \t Accuracy 0.70\n",
      "Epoch 10. [Test] \t Time 7.72 T-Loss                 0.59 \t T-Accuracy 0.68\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "\n",
    "experiment(num_epochs=num_epochs,\n",
    "           trainloader=train_trainloader,\n",
    "           device=device,\n",
    "           model=model,\n",
    "           optimizer=optimizer,\n",
    "           scheduler=lr_scheduler)\n",
    "\n",
    "# 70/30 split\n",
    "# Epoch 1. [Train] \t Time 0.69 Loss                 0.71 \t Accuracy 0.50\n",
    "# Epoch 1. [Test] \t Time 0.52 T-Loss                 0.69 \t T-Accuracy 0.50\n",
    "# Epoch 2. [Train] \t Time 0.59 Loss                 0.69 \t Accuracy 0.52\n",
    "# Epoch 2. [Test] \t Time 0.52 T-Loss                 0.69 \t T-Accuracy 0.53\n",
    "# Epoch 3. [Train] \t Time 0.58 Loss                 0.67 \t Accuracy 0.59\n",
    "# Epoch 3. [Test] \t Time 0.52 T-Loss                 0.69 \t T-Accuracy 0.57\n",
    "# Epoch 4. [Train] \t Time 0.58 Loss                 0.61 \t Accuracy 0.67\n",
    "# Epoch 4. [Test] \t Time 0.52 T-Loss                 0.67 \t T-Accuracy 0.60\n",
    "# Epoch 5. [Train] \t Time 0.58 Loss                 0.49 \t Accuracy 0.77\n",
    "# Epoch 5. [Test] \t Time 0.52 T-Loss                 0.68 \t T-Accuracy 0.64\n",
    "# Epoch 6. [Train] \t Time 0.58 Loss                 0.23 \t Accuracy 0.92\n",
    "# Epoch 6. [Test] \t Time 0.51 T-Loss                 0.80 \t T-Accuracy 0.68\n",
    "# Epoch 7. [Train] \t Time 0.58 Loss                 0.11 \t Accuracy 0.96\n",
    "# Epoch 7. [Test] \t Time 0.52 T-Loss                 0.94 \t T-Accuracy 0.69\n",
    "# Epoch 8. [Train] \t Time 0.58 Loss                 0.07 \t Accuracy 0.98\n",
    "# Epoch 8. [Test] \t Time 0.52 T-Loss                 1.04 \t T-Accuracy 0.70\n",
    "# Epoch 9. [Train] \t Time 0.58 Loss                 0.04 \t Accuracy 0.99\n",
    "# Epoch 9. [Test] \t Time 0.52 T-Loss                 1.15 \t T-Accuracy 0.70\n",
    "# Epoch 10. [Train] \t Time 0.59 Loss                 0.04 \t Accuracy 0.99\n",
    "# Epoch 10. [Test] \t Time 0.52 T-Loss                 1.20 \t T-Accuracy 0.70\n",
    "# Epoch 11. [Train] \t Time 0.58 Loss                 0.03 \t Accuracy 0.99\n",
    "# Epoch 11. [Test] \t Time 0.52 T-Loss                 1.19 \t T-Accuracy 0.71"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting on test_data...\n",
      "Predicted: 0/59544, Time elapsed: 00:00:00.13\n",
      "Predicted: 1000/59544, Time elapsed: 00:00:01.20\n",
      "Predicted: 2000/59544, Time elapsed: 00:00:02.28\n",
      "Predicted: 3000/59544, Time elapsed: 00:00:03.35\n",
      "Predicted: 4000/59544, Time elapsed: 00:00:04.46\n",
      "Predicted: 5000/59544, Time elapsed: 00:00:05.57\n",
      "Predicted: 6000/59544, Time elapsed: 00:00:06.65\n",
      "Predicted: 7000/59544, Time elapsed: 00:00:07.67\n",
      "Predicted: 8000/59544, Time elapsed: 00:00:08.78\n",
      "Predicted: 9000/59544, Time elapsed: 00:00:09.83\n",
      "Predicted: 10000/59544, Time elapsed: 00:00:10.88\n",
      "Predicted: 11000/59544, Time elapsed: 00:00:11.96\n",
      "Predicted: 12000/59544, Time elapsed: 00:00:13.04\n",
      "Predicted: 13000/59544, Time elapsed: 00:00:14.14\n",
      "Predicted: 14000/59544, Time elapsed: 00:00:15.16\n",
      "Predicted: 15000/59544, Time elapsed: 00:00:16.19\n",
      "Predicted: 16000/59544, Time elapsed: 00:00:17.28\n",
      "Predicted: 17000/59544, Time elapsed: 00:00:18.34\n",
      "Predicted: 18000/59544, Time elapsed: 00:00:19.41\n",
      "Predicted: 19000/59544, Time elapsed: 00:00:20.50\n",
      "Predicted: 20000/59544, Time elapsed: 00:00:21.57\n",
      "Predicted: 21000/59544, Time elapsed: 00:00:22.62\n",
      "Predicted: 22000/59544, Time elapsed: 00:00:23.79\n",
      "Predicted: 23000/59544, Time elapsed: 00:00:24.88\n",
      "Predicted: 24000/59544, Time elapsed: 00:00:25.92\n",
      "Predicted: 25000/59544, Time elapsed: 00:00:26.97\n",
      "Predicted: 26000/59544, Time elapsed: 00:00:28.01\n",
      "Predicted: 27000/59544, Time elapsed: 00:00:29.05\n",
      "Predicted: 28000/59544, Time elapsed: 00:00:30.16\n",
      "Predicted: 29000/59544, Time elapsed: 00:00:31.25\n",
      "Predicted: 30000/59544, Time elapsed: 00:00:32.26\n",
      "Predicted: 31000/59544, Time elapsed: 00:00:33.29\n",
      "Predicted: 32000/59544, Time elapsed: 00:00:34.32\n",
      "Predicted: 33000/59544, Time elapsed: 00:00:35.34\n",
      "Predicted: 34000/59544, Time elapsed: 00:00:36.40\n",
      "Predicted: 35000/59544, Time elapsed: 00:00:37.46\n",
      "Predicted: 36000/59544, Time elapsed: 00:00:38.57\n",
      "Predicted: 37000/59544, Time elapsed: 00:00:39.69\n",
      "Predicted: 38000/59544, Time elapsed: 00:00:40.79\n",
      "Predicted: 39000/59544, Time elapsed: 00:00:41.90\n",
      "Predicted: 40000/59544, Time elapsed: 00:00:43.03\n",
      "Predicted: 41000/59544, Time elapsed: 00:00:44.08\n",
      "Predicted: 42000/59544, Time elapsed: 00:00:45.21\n",
      "Predicted: 43000/59544, Time elapsed: 00:00:46.24\n",
      "Predicted: 44000/59544, Time elapsed: 00:00:47.31\n",
      "Predicted: 45000/59544, Time elapsed: 00:00:48.42\n",
      "Predicted: 46000/59544, Time elapsed: 00:00:49.47\n",
      "Predicted: 47000/59544, Time elapsed: 00:00:50.55\n",
      "Predicted: 48000/59544, Time elapsed: 00:00:51.62\n",
      "Predicted: 49000/59544, Time elapsed: 00:00:52.65\n",
      "Predicted: 50000/59544, Time elapsed: 00:00:53.75\n",
      "Predicted: 51000/59544, Time elapsed: 00:00:54.80\n",
      "Predicted: 52000/59544, Time elapsed: 00:00:55.85\n",
      "Predicted: 53000/59544, Time elapsed: 00:00:56.88\n",
      "Predicted: 54000/59544, Time elapsed: 00:00:57.92\n",
      "Predicted: 55000/59544, Time elapsed: 00:00:58.97\n",
      "Predicted: 56000/59544, Time elapsed: 00:00:60.00\n",
      "Predicted: 57000/59544, Time elapsed: 00:01:01.10\n",
      "Predicted: 58000/59544, Time elapsed: 00:01:02.14\n",
      "Predicted: 59000/59544, Time elapsed: 00:01:03.19\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "def timer(start, end):\n",
    "    hours, rem = divmod(end-start, 3600)\n",
    "    minutes, seconds = divmod(rem, 60)\n",
    "    return \"{:0>2}:{:0>2}:{:05.2f}\".format(int(hours), int(minutes), seconds)\n",
    "\n",
    "predictions = np.zeros(len(test_data))\n",
    "print(\"predicting on test_data...\")\n",
    "l_test = len(test_data)\n",
    "start_t = time.time()\n",
    "\n",
    "# Set model to evaluating mode\n",
    "model.eval()\n",
    "\n",
    "for i, data in enumerate(test_dataloader, 0):\n",
    "    x = data[\"x\"].float()\n",
    "    \n",
    "    x = x.to(device)\n",
    "    predictions[i] = model(x)\n",
    "    # predictions[i] = model_pred(x)\n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted: {i}/{l_test}, Time elapsed: {timer(start_t, time.time())}\")\n",
    "        \n",
    "        \n",
    "# predictions_INT = pd.DataFrame(predictions).astype(int)\n",
    "predictions_DF = pd.DataFrame(predictions)\n",
    "predictions_rounded = predictions_DF.round(0)\n",
    "\n",
    "# print(\"saving predictions...\")\n",
    "# predictions_INT.to_csv('data/predictions.csv', index=False, header=False)\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saving predictions...\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   1\n",
       "1   0\n",
       "2   1\n",
       "3   0\n",
       "4   1\n",
       ".. ..\n",
       "95  1\n",
       "96  1\n",
       "97  1\n",
       "98  0\n",
       "99  1\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# res = pd.DataFrame(predictions_rounded).astype(int)\n",
    "res = pd.DataFrame(predictions_rounded).astype(int)\n",
    "print(\"saving predictions...\")\n",
    "res.to_csv('data/predictions_inverse_cooking_10_epochs_batchs_10.csv', index=False, header=False)\n",
    "print(\"done\")\n",
    "\n",
    "res.head(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Path(\"model\").mkdir(parents=True, exist_ok=True)  # create folder if necessary\n",
    "torch.save(model.state_dict(), \"model/params_6_layers_mobileNetV3small_v2\")\n",
    "# load: model.load_state_dict(torch.load(\"model/params\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
