{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, Subset, DataLoader\n",
    "# from torchvision import datasets, models, transforms\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "num_epochs = 30  # each epoch is one pass over the whole dataset\n",
    "batch_size = 128  # how many samples per training step (32,64,128)\n",
    "num_workers = 7  # how many CPU cores to use\n",
    "\n",
    "n_features = 1000 # How many features per image\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "class FoodTuples(Dataset):\n",
    "    \"\"\"\n",
    "    Class to load food tuples. Individual items consist of:\n",
    "     - x: concatenated image features of a tuple (AB)\n",
    "     - y: corresponding label (1: is similar 0: not similar, None: no label)\n",
    "    \"\"\"\n",
    "    def __init__(self, features_file, triplets_file):\n",
    "        print(\"initializing \" + str(triplets_file) + \" dataset...\")\n",
    "        self.img_features = pd.read_csv(\"data/\" + features_file, header=None, index_col=0)\n",
    "        self.triplets = pd.read_csv(\"data/\" + triplets_file, sep=\" \", header=None).to_numpy()\n",
    "        \n",
    "        n_triplets = self.triplets.shape[0]\n",
    "        n_features = self.img_features.shape[1]\n",
    "        \n",
    "        self.tuples = np.zeros((2 * n_triplets, 2 * n_features ), dtype=np.int32)\n",
    "        self.labels = np.zeros((2 * n_triplets, 1 ), dtype=np.bool)\n",
    "        \n",
    "         \n",
    "        # For each triplet add 2  variations\n",
    "        idx = 0\n",
    "        for triplet in  self.triplets:\n",
    "            a, b, c = triplet[0], triplet[1], triplet[2]\n",
    "            a_features = self.img_features.loc[[a]].to_numpy(dtype=np.float32)\n",
    "            b_features = self.img_features.loc[[b]].to_numpy(dtype=np.float32)\n",
    "            c_features = self.img_features.loc[[c]].to_numpy(dtype=np.float32)\n",
    "\n",
    "            # AB = 1, BA = 1\n",
    "            if np.random.random() < 0.5:\n",
    "                self.tuples[idx, :] =  np.squeeze(np.concatenate((a_features, b_features), axis=1))\n",
    "                self.labels[idx,0] = 1\n",
    "            else:\n",
    "                self.tuples[idx, :] =  np.squeeze(np.concatenate((b_features, a_features), axis=1))\n",
    "                self.labels[idx,0] = 1\n",
    "\n",
    "            # AC = 0, CA = 0\n",
    "            if np.random.random() < 0.5:\n",
    "                self.tuples[idx+1, :] =  np.squeeze(np.concatenate((c_features, a_features), axis=1))\n",
    "                self.labels[idx+1,0] = 0\n",
    "            else:\n",
    "                self.tuples[idx+1, :] =  np.squeeze(np.concatenate((a_features, c_features), axis=1))\n",
    "                self.labels[idx+1,0] = 0\n",
    "            idx += 2\n",
    "\n",
    "        print(\"done\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.tuples.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        features = self.tuples[idx]\n",
    "        label = self.labels[idx]\n",
    "        return {\"x\": features, \"y\": label}\n",
    "    \n",
    " \n",
    "    \n",
    "class FoodTriplets(Dataset):\n",
    "    \"\"\"\n",
    "    Class to load food triplets. Individual items consist of:\n",
    "     - x: concatenated image features of a triplet (ABC)\n",
    "     - y: corresponding label (1: B is more similar, 0: C is more similar, None: no label)\n",
    "    \"\"\"\n",
    "    def __init__(self, features_file, triplets_file, is_labelled_data=False, train=False, extend=False):\n",
    "        print(\"initializing \" + str(triplets_file) + \" dataset...\")\n",
    "        self.img_features = pd.read_csv(\"data/\" + features_file, header=None, index_col=0)\n",
    "        self.triplets = pd.read_csv(\"data/\" + triplets_file, sep=\" \", header=None).to_numpy()\n",
    "        self.is_labelled_data = is_labelled_data\n",
    "        self.labels = None\n",
    "        self.extend = extend\n",
    " \n",
    "        print(\"done\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.triplets)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        triplet = self.triplets[idx]\n",
    "        a, b, c = triplet[0], triplet[1], triplet[2]\n",
    "        a_features = self.img_features.loc[[a]].to_numpy(dtype=np.float32)\n",
    "        b_features = self.img_features.loc[[b]].to_numpy(dtype=np.float32)\n",
    "        c_features = self.img_features.loc[[c]].to_numpy(dtype=np.float32)\n",
    "        features = np.squeeze(np.concatenate((a_features, b_features, c_features), axis=1))\n",
    "        label = 0  # dummy label\n",
    "        if self.is_labelled_data:\n",
    "            label = np.array([self.labels[idx]])\n",
    "        return {\"x\": features, \"y\": label}\n",
    "    \n",
    " \n",
    "\n",
    " \n",
    "class SimilarityNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # 2000\n",
    "        # 1000\n",
    "        # 500\n",
    "        # 250\n",
    "        # 1\n",
    "        \n",
    "        nin = 2000\n",
    "        n1 = 1000\n",
    "        n2 = 500\n",
    "        n3 = 250\n",
    "#         n4 = int(n_features/4)\n",
    "#         n5 = int(n_features/8)\n",
    "        nout = 1\n",
    "\n",
    "        p_dropout1 = 0.3\n",
    "        p_dropout2 = 0.5\n",
    "        p_dropout3 = 0.5\n",
    "        p_dropout4 = 0.5\n",
    "        p_dropout5 = 0.2\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=nin, out_features=n1),\n",
    "            nn.BatchNorm1d(n1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout1, inplace=False),\n",
    "            nn.Linear(in_features=n1, out_features=n2),\n",
    "            nn.BatchNorm1d(n2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout2, inplace=False),\n",
    "            nn.Linear(in_features=n2, out_features=n3),\n",
    "            nn.BatchNorm1d(n3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout3, inplace=False),\n",
    "            nn.Linear(in_features=n3, out_features=nout),\n",
    "#             nn.BatchNorm1d(n4),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.Dropout(p=p_dropout4, inplace=False),\n",
    "#             nn.Linear(in_features=n4, out_features=n5),\n",
    "#             nn.ReLU(inplace=True),\n",
    "#             nn.BatchNorm1d(n5),\n",
    "#             nn.Dropout(p=p_dropout5, inplace=False),\n",
    "#             nn.Linear(in_features=n5, out_features=nout),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "def get_model(device):\n",
    "    return SimpleNetwork().to(device)\n",
    "\n",
    "class SimpleNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        nin = 3*n_features\n",
    "        n1 = 2*n_features\n",
    "        n2 = n_features\n",
    "        n3 = int(n_features/2)\n",
    "        n4 = int(n_features/4)\n",
    "        n5 = int(n_features/8)\n",
    "        nout = 1\n",
    "\n",
    "        p_dropout1 = 0.3\n",
    "        p_dropout2 = 0.5\n",
    "        p_dropout3 = 0.5\n",
    "        p_dropout4 = 0.5\n",
    "        p_dropout5 = 0.2\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=nin, out_features=n1),\n",
    "            nn.BatchNorm1d(n1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout1, inplace=False),\n",
    "            nn.Linear(in_features=n1, out_features=n2),\n",
    "            nn.BatchNorm1d(n2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout2, inplace=False),\n",
    "            nn.Linear(in_features=n2, out_features=n3),\n",
    "            nn.BatchNorm1d(n3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout3, inplace=False),\n",
    "            nn.Linear(in_features=n3, out_features=n4),\n",
    "            nn.BatchNorm1d(n4),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(p=p_dropout4, inplace=False),\n",
    "            nn.Linear(in_features=n4, out_features=n5),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.BatchNorm1d(n5),\n",
    "            nn.Dropout(p=p_dropout5, inplace=False),\n",
    "            nn.Linear(in_features=n5, out_features=nout),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        \n",
    "      \n",
    "    def forward(self, x):\n",
    "        logits = self.classifier(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initializing train_triplets.txt dataset...\n",
      "done\n",
      "initializing test_triplets.txt dataset...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# Specify some file names\n",
    "image_features_file = \"train_image_features_mobilenet_v3_small.csv\"\n",
    "train_triplets_file = \"train_triplets.txt\" #\"train_triplets.txt\"\n",
    "test_triplets_file = \"test_triplets.txt\" #\"test_triplets.txt\"\n",
    "\n",
    " \n",
    "# initialize datasets\n",
    "# train_data = FoodTriplets(image_features_file, train_triplets_file, is_labelled_data=True, train=True, extend=False)\n",
    "# test_data = FoodTriplets(image_features_file, test_triplets_file, is_labelled_data=False, train=False, extend=False)\n",
    "\n",
    "# initialize datasets\n",
    "train_data = FoodTuples(image_features_file, train_triplets_file)\n",
    "\n",
    "test_data = FoodTriplets(image_features_file, test_triplets_file, is_labelled_data=False, train=False, extend=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Split train set into train and test set to assess accuracy on unused set\n",
    "l_train = len(train_data)\n",
    "val_size = int(0.7 * l_train + 1)\n",
    "indices = list(range(l_train))\n",
    "np.random.shuffle(indices)\n",
    "val_indices, t_indices = indices[:val_size], indices[val_size:]\n",
    "\n",
    "len(val_indices)\n",
    "len(t_indices)\n",
    "\n",
    "\n",
    "train_trainloader = DataLoader(torch.utils.data.Subset(train_data, t_indices), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "train_testloader = DataLoader(torch.utils.data.Subset(train_data, val_indices), batch_size=batch_size, shuffle=True, num_workers=num_workers)\n",
    "\n",
    "test_dataloader = DataLoader(test_data, batch_size=1, shuffle=False, num_workers=num_workers)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# device = 'cpu'\n",
    "\n",
    "model = SimilarityNetwork().to(device)\n",
    "\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "# optimizer = optim.SGD(model.parameters(), lr=0.1, momentum=0.01, nesterov=True)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 \t Train_acc: 0.884 \t Test_acc: 0.580\n",
      "Epoch 1 \t Train_acc: 0.993 \t Test_acc: 0.627\n",
      "Epoch 2 \t Train_acc: 0.980 \t Test_acc: 0.618\n",
      "Epoch 3 \t Train_acc: 0.945 \t Test_acc: 0.609\n",
      "Epoch 4 \t Train_acc: 0.733 \t Test_acc: 0.570\n",
      "Epoch 5 \t Train_acc: 0.954 \t Test_acc: 0.604\n",
      "Epoch 6 \t Train_acc: 0.919 \t Test_acc: 0.591\n",
      "Epoch 7 \t Train_acc: 0.982 \t Test_acc: 0.633\n",
      "Epoch 8 \t Train_acc: 0.979 \t Test_acc: 0.621\n",
      "Epoch 9 \t Train_acc: 0.963 \t Test_acc: 0.607\n",
      "Epoch 10 \t Train_acc: 0.993 \t Test_acc: 0.622\n",
      "Epoch 11 \t Train_acc: 0.948 \t Test_acc: 0.601\n",
      "Epoch 12 \t Train_acc: 0.926 \t Test_acc: 0.600\n",
      "Epoch 13 \t Train_acc: 0.896 \t Test_acc: 0.602\n",
      "Epoch 14 \t Train_acc: 0.983 \t Test_acc: 0.618\n",
      "Epoch 15 \t Train_acc: 0.946 \t Test_acc: 0.603\n",
      "Epoch 16 \t Train_acc: 0.958 \t Test_acc: 0.613\n",
      "Epoch 17 \t Train_acc: 0.971 \t Test_acc: 0.604\n",
      "Epoch 18 \t Train_acc: 0.981 \t Test_acc: 0.610\n",
      "Epoch 19 \t Train_acc: 0.985 \t Test_acc: 0.625\n",
      "Epoch 20 \t Train_acc: 0.679 \t Test_acc: 0.534\n",
      "Epoch 21 \t Train_acc: 0.969 \t Test_acc: 0.604\n",
      "Epoch 22 \t Train_acc: 0.991 \t Test_acc: 0.628\n",
      "Epoch 23 \t Train_acc: 0.948 \t Test_acc: 0.603\n",
      "Epoch 24 \t Train_acc: 0.851 \t Test_acc: 0.572\n",
      "Epoch 25 \t Train_acc: 0.972 \t Test_acc: 0.630\n",
      "Epoch 26 \t Train_acc: 0.973 \t Test_acc: 0.610\n",
      "Epoch 27 \t Train_acc: 0.976 \t Test_acc: 0.616\n",
      "Epoch 28 \t Train_acc: 0.972 \t Test_acc: 0.630\n",
      "Epoch 29 \t Train_acc: 0.995 \t Test_acc: 0.623\n",
      "Epoch 30 \t Train_acc: 0.922 \t Test_acc: 0.611\n",
      "Epoch 31 \t Train_acc: 0.870 \t Test_acc: 0.581\n",
      "Epoch 32 \t Train_acc: 0.960 \t Test_acc: 0.603\n",
      "Epoch 33 \t Train_acc: 0.968 \t Test_acc: 0.616\n",
      "Epoch 34 \t Train_acc: 0.984 \t Test_acc: 0.618\n",
      "Epoch 35 \t Train_acc: 0.822 \t Test_acc: 0.564\n",
      "Epoch 36 \t Train_acc: 0.973 \t Test_acc: 0.602\n",
      "Epoch 37 \t Train_acc: 0.993 \t Test_acc: 0.621\n",
      "Epoch 38 \t Train_acc: 0.974 \t Test_acc: 0.628\n",
      "Epoch 39 \t Train_acc: 0.973 \t Test_acc: 0.603\n",
      "Epoch 40 \t Train_acc: 0.989 \t Test_acc: 0.617\n",
      "Epoch 41 \t Train_acc: 0.994 \t Test_acc: 0.627\n",
      "Epoch 42 \t Train_acc: 0.943 \t Test_acc: 0.600\n",
      "Epoch 43 \t Train_acc: 0.981 \t Test_acc: 0.630\n",
      "Epoch 44 \t Train_acc: 0.982 \t Test_acc: 0.634\n",
      "Epoch 45 \t Train_acc: 0.968 \t Test_acc: 0.612\n",
      "Epoch 46 \t Train_acc: 0.933 \t Test_acc: 0.627\n",
      "Epoch 47 \t Train_acc: 0.996 \t Test_acc: 0.615\n",
      "Epoch 48 \t Train_acc: 0.974 \t Test_acc: 0.629\n",
      "Epoch 49 \t Train_acc: 0.977 \t Test_acc: 0.609\n",
      "Epoch 50 \t Train_acc: 0.892 \t Test_acc: 0.578\n",
      "Epoch 51 \t Train_acc: 0.985 \t Test_acc: 0.630\n",
      "Epoch 52 \t Train_acc: 0.990 \t Test_acc: 0.622\n",
      "Epoch 53 \t Train_acc: 0.923 \t Test_acc: 0.592\n",
      "Epoch 54 \t Train_acc: 0.965 \t Test_acc: 0.628\n",
      "Epoch 55 \t Train_acc: 0.992 \t Test_acc: 0.613\n",
      "Epoch 56 \t Train_acc: 0.983 \t Test_acc: 0.605\n",
      "Epoch 57 \t Train_acc: 0.986 \t Test_acc: 0.612\n",
      "Epoch 58 \t Train_acc: 0.967 \t Test_acc: 0.625\n",
      "Epoch 59 \t Train_acc: 0.947 \t Test_acc: 0.603\n",
      "Epoch 60 \t Train_acc: 0.993 \t Test_acc: 0.612\n",
      "Epoch 61 \t Train_acc: 0.951 \t Test_acc: 0.620\n",
      "Epoch 62 \t Train_acc: 0.987 \t Test_acc: 0.625\n",
      "Epoch 63 \t Train_acc: 0.993 \t Test_acc: 0.629\n",
      "Epoch 64 \t Train_acc: 0.990 \t Test_acc: 0.628\n",
      "Epoch 65 \t Train_acc: 0.996 \t Test_acc: 0.618\n",
      "Epoch 66 \t Train_acc: 0.893 \t Test_acc: 0.615\n",
      "Epoch 67 \t Train_acc: 0.992 \t Test_acc: 0.615\n",
      "Epoch 68 \t Train_acc: 0.922 \t Test_acc: 0.592\n",
      "Epoch 69 \t Train_acc: 0.991 \t Test_acc: 0.632\n",
      "Epoch 70 \t Train_acc: 0.990 \t Test_acc: 0.610\n",
      "Epoch 71 \t Train_acc: 0.921 \t Test_acc: 0.596\n",
      "Epoch 72 \t Train_acc: 0.978 \t Test_acc: 0.605\n",
      "Epoch 73 \t Train_acc: 0.978 \t Test_acc: 0.612\n",
      "Epoch 74 \t Train_acc: 0.986 \t Test_acc: 0.611\n",
      "Epoch 75 \t Train_acc: 0.856 \t Test_acc: 0.570\n",
      "Epoch 76 \t Train_acc: 0.990 \t Test_acc: 0.629\n",
      "Epoch 77 \t Train_acc: 0.961 \t Test_acc: 0.630\n",
      "Epoch 78 \t Train_acc: 0.995 \t Test_acc: 0.626\n",
      "Epoch 79 \t Train_acc: 0.994 \t Test_acc: 0.625\n",
      "Epoch 80 \t Train_acc: 0.990 \t Test_acc: 0.609\n",
      "Epoch 81 \t Train_acc: 0.991 \t Test_acc: 0.618\n",
      "Epoch 82 \t Train_acc: 0.978 \t Test_acc: 0.613\n",
      "Epoch 83 \t Train_acc: 0.974 \t Test_acc: 0.614\n",
      "Epoch 84 \t Train_acc: 0.985 \t Test_acc: 0.626\n",
      "Epoch 85 \t Train_acc: 0.970 \t Test_acc: 0.622\n",
      "Epoch 86 \t Train_acc: 0.998 \t Test_acc: 0.623\n",
      "Epoch 87 \t Train_acc: 0.934 \t Test_acc: 0.588\n",
      "Epoch 88 \t Train_acc: 0.987 \t Test_acc: 0.632\n",
      "Epoch 89 \t Train_acc: 0.991 \t Test_acc: 0.614\n",
      "Epoch 90 \t Train_acc: 0.955 \t Test_acc: 0.593\n",
      "Epoch 91 \t Train_acc: 0.996 \t Test_acc: 0.628\n",
      "Epoch 92 \t Train_acc: 0.998 \t Test_acc: 0.620\n",
      "Epoch 93 \t Train_acc: 0.996 \t Test_acc: 0.621\n",
      "Epoch 94 \t Train_acc: 0.997 \t Test_acc: 0.618\n",
      "Epoch 95 \t Train_acc: 0.932 \t Test_acc: 0.587\n",
      "Epoch 96 \t Train_acc: 0.922 \t Test_acc: 0.600\n",
      "Epoch 97 \t Train_acc: 0.990 \t Test_acc: 0.637\n",
      "Epoch 98 \t Train_acc: 0.913 \t Test_acc: 0.587\n",
      "Epoch 99 \t Train_acc: 0.969 \t Test_acc: 0.636\n"
     ]
    }
   ],
   "source": [
    "model.train()\n",
    "\n",
    "num_epochs = 100  # each epoch is one pass over the whole dataset\n",
    "\n",
    "for epoch in range(0,num_epochs):\n",
    "    \n",
    "    \n",
    "    # TRAINING\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = []\n",
    "\n",
    "    for i, data in enumerate(train_trainloader, 1):\n",
    "        inputs =  data[\"x\"].float()\n",
    "        labels =  data[\"y\"].float()\n",
    "        \n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "                \n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()  # all the tensors have .grad attribute\n",
    "        \n",
    "        # forward propagation\n",
    "        logits = model(inputs) # forward propagation\n",
    "        loss = criterion(logits, labels) # computing the loss for predictions\n",
    "        \n",
    "        # Backward propagation\n",
    "        loss.backward() # backpropgation\n",
    "        # Optimization step.\n",
    "        optimizer.step() # applying an optimization step\n",
    "        \n",
    "        train_loss.append(loss)\n",
    " \n",
    "    \n",
    "    \n",
    "    # ACCURACY ON TRAINING DATA\n",
    "    model.eval()\n",
    "    \n",
    "    train_accuracy = []\n",
    "    \n",
    "    for i, data in enumerate(train_trainloader, 1):\n",
    "#         end = time.time()\n",
    "        \n",
    "        inputs =  data[\"x\"].float()\n",
    "        labels =  data[\"y\"].float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        bs = inputs.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "\n",
    "            acc = ((torch.round(logits) == labels).sum().float() / bs).float()\n",
    "            train_accuracy.append(acc)\n",
    "    \n",
    "    # TESTING\n",
    "    model.eval()\n",
    "    \n",
    "    test_accuracy = []\n",
    "    \n",
    "    for i, data in enumerate(train_testloader, 1):        \n",
    "        inputs =  data[\"x\"].float()\n",
    "        labels =  data[\"y\"].float()\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        bs = inputs.size(0)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            \n",
    "            acc = ((torch.round(logits) == labels).sum().float() / bs).float()\n",
    "            test_accuracy.append(acc)\n",
    "\n",
    "    train_accuracy_epoch = torch.mean(torch.stack(train_accuracy)).item()\n",
    "    test_accuracy_epoch = torch.mean(torch.stack(test_accuracy)).item()\n",
    "    \n",
    "    print(f'Epoch {epoch} \\t Train_acc: {train_accuracy_epoch:.3f} \\t Test_acc: {test_accuracy_epoch:.3f}')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: 0/59544\n",
      "Predicted: 1000/59544\n",
      "Predicted: 2000/59544\n",
      "Predicted: 3000/59544\n",
      "Predicted: 4000/59544\n",
      "Predicted: 5000/59544\n",
      "Predicted: 6000/59544\n",
      "Predicted: 7000/59544\n",
      "Predicted: 8000/59544\n",
      "Predicted: 9000/59544\n",
      "Predicted: 10000/59544\n",
      "Predicted: 11000/59544\n",
      "Predicted: 12000/59544\n",
      "Predicted: 13000/59544\n",
      "Predicted: 14000/59544\n",
      "Predicted: 15000/59544\n",
      "Predicted: 16000/59544\n",
      "Predicted: 17000/59544\n",
      "Predicted: 18000/59544\n",
      "Predicted: 19000/59544\n",
      "Predicted: 20000/59544\n",
      "Predicted: 21000/59544\n",
      "Predicted: 22000/59544\n",
      "Predicted: 23000/59544\n",
      "Predicted: 24000/59544\n",
      "Predicted: 25000/59544\n",
      "Predicted: 26000/59544\n",
      "Predicted: 27000/59544\n",
      "Predicted: 28000/59544\n",
      "Predicted: 29000/59544\n",
      "Predicted: 30000/59544\n",
      "Predicted: 31000/59544\n",
      "Predicted: 32000/59544\n",
      "Predicted: 33000/59544\n",
      "Predicted: 34000/59544\n",
      "Predicted: 35000/59544\n",
      "Predicted: 36000/59544\n",
      "Predicted: 37000/59544\n",
      "Predicted: 38000/59544\n",
      "Predicted: 39000/59544\n",
      "Predicted: 40000/59544\n",
      "Predicted: 41000/59544\n",
      "Predicted: 42000/59544\n",
      "Predicted: 43000/59544\n",
      "Predicted: 44000/59544\n",
      "Predicted: 45000/59544\n",
      "Predicted: 46000/59544\n",
      "Predicted: 47000/59544\n",
      "Predicted: 48000/59544\n",
      "Predicted: 49000/59544\n",
      "Predicted: 50000/59544\n",
      "Predicted: 51000/59544\n",
      "Predicted: 52000/59544\n",
      "Predicted: 53000/59544\n",
      "Predicted: 54000/59544\n",
      "Predicted: 55000/59544\n",
      "Predicted: 56000/59544\n",
      "Predicted: 57000/59544\n",
      "Predicted: 58000/59544\n",
      "Predicted: 59000/59544\n",
      "saving predictions...\n",
      "done\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    0\n",
       "0   1\n",
       "1   0\n",
       "2   1\n",
       "3   0\n",
       "4   1\n",
       ".. ..\n",
       "95  1\n",
       "96  1\n",
       "97  0\n",
       "98  1\n",
       "99  0\n",
       "\n",
       "[100 rows x 1 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "n_testdata = len(test_data)\n",
    "predictions = np.zeros(n_testdata)\n",
    "\n",
    "\n",
    "for i, data in enumerate(test_dataloader): \n",
    "    \n",
    "    inputs =  data[\"x\"].float()\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    A_tensor = inputs[0][:1000]\n",
    "    B_tensor = inputs[0][1000:2000]\n",
    "    C_tensor = inputs[0][2000:3000]\n",
    "    \n",
    "    \n",
    "    # AB\n",
    "    AB_tensor = torch.cat((A_tensor, B_tensor), 0)\n",
    "    AB_tensor = AB_tensor.reshape(1,-1)\n",
    "        \n",
    "#     print(AB_tensor.shape)\n",
    "    \n",
    "    AC_tensor = torch.cat((A_tensor, C_tensor), 0)\n",
    "    AC_tensor = AC_tensor.reshape(1,-1)\n",
    "\n",
    "    output_AB = model(AB_tensor)\n",
    "    output_AC = model(AC_tensor)\n",
    "    \n",
    "#     print(output_AB[0][0].item(),output_AC[0][0].item())\n",
    "    \n",
    "#     if i > 20:\n",
    "#         break\n",
    "#     break\n",
    "    if output_AB > output_AC:\n",
    "        predictions[i] = 1\n",
    "#         print('AB')\n",
    "    else:\n",
    "#         print('AC')\n",
    "        predictions[i] = 0\n",
    "    \n",
    "    if i % 1000 == 0:\n",
    "        print(f\"Predicted: {i}/{n_testdata}\")\n",
    "        \n",
    "         \n",
    "        \n",
    " \n",
    "# res = pd.DataFrame(predictions).astype(int)\n",
    "res = pd.DataFrame(predictions).astype(int)\n",
    "print(\"saving predictions...\")\n",
    "res.to_csv('data/predictions_mobilenet_AB_AC.csv', index=False, header=False)\n",
    "print(\"done\")\n",
    "\n",
    "res.head(100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
